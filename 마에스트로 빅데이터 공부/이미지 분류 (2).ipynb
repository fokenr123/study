{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"C:/Users/ICT01_20/Desktop/food-5\"\n",
    "categories  = os.listdir(caltech_dir)\n",
    "nb_classes = len(categories )\n",
    "image_w = 128\n",
    "image_h = 128\n",
    "\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계란찜  파일 길이 :  784\n",
      "계란찜  :  C:/Users/ICT01_20/Desktop/food-5/계란찜\\Img_125_0000.jpg\n",
      "계란찜  :  C:/Users/ICT01_20/Desktop/food-5/계란찜\\Img_125_0871.jpg\n",
      "고사리나물  파일 길이 :  896\n",
      "고사리나물  :  C:/Users/ICT01_20/Desktop/food-5/고사리나물\\Img_041_0000.JPG\n",
      "고사리나물  :  C:/Users/ICT01_20/Desktop/food-5/고사리나물\\Img_041_0782.jpg\n",
      "삼계탕  파일 길이 :  864\n",
      "삼계탕  :  C:/Users/ICT01_20/Desktop/food-5/삼계탕\\Img_138_0000.jpg\n",
      "삼계탕  :  C:/Users/ICT01_20/Desktop/food-5/삼계탕\\Img_138_0785.jpg\n",
      "오징어채볶음  파일 길이 :  704\n",
      "오징어채볶음  :  C:/Users/ICT01_20/Desktop/food-5/오징어채볶음\\Img_087_0003.jpg\n",
      "오징어채볶음  :  C:/Users/ICT01_20/Desktop/food-5/오징어채볶음\\Img_087_1070.jpg\n",
      "콩자반  파일 길이 :  675\n",
      "콩자반  :  C:/Users/ICT01_20/Desktop/food-5/콩자반\\Img_025_0001.jpg\n",
      "ok 3923\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.JPG\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "train_ratio = 0.60\n",
    "validation_ratio = 0.20\n",
    "test_ratio = 0.20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
    "xy = (X_train, X_test, X_val , y_train, y_test,y_val )\n",
    "np.save(\"C:/Users/ICT01_20/Desktop/food-5-2\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 128, 128, 3)\n",
      "(2353, 128, 128, 3)\n",
      "(785, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]]\n",
      "\n",
      "\n",
      " [[[0.00364475 0.00359862 0.00333718]\n",
      "   [0.00370627 0.00366013 0.00344483]\n",
      "   [0.00373702 0.00370627 0.0035371 ]\n",
      "   ...\n",
      "   [0.00324491 0.00299885 0.00249135]\n",
      "   [0.00324491 0.00301423 0.00250673]\n",
      "   [0.00322953 0.00298347 0.00249135]]\n",
      "\n",
      "  [[0.00366013 0.00362937 0.00338331]\n",
      "   [0.00370627 0.00367551 0.00347559]\n",
      "   [0.00372165 0.00372165 0.00355248]\n",
      "   ...\n",
      "   [0.00324491 0.00299885 0.00247597]\n",
      "   [0.00324491 0.00299885 0.00249135]\n",
      "   [0.00324491 0.00296809 0.00246059]]\n",
      "\n",
      "  [[0.00369089 0.00366013 0.00342945]\n",
      "   [0.00372165 0.00369089 0.00350634]\n",
      "   [0.00372165 0.00372165 0.00355248]\n",
      "   ...\n",
      "   [0.00321415 0.00293733 0.00239908]\n",
      "   [0.00322953 0.00293733 0.00242983]\n",
      "   [0.00322953 0.00292195 0.00242983]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00362937 0.00358324 0.0033218 ]\n",
      "   [0.00364475 0.00359862 0.00336794]\n",
      "   [0.00364475 0.00358324 0.00339869]\n",
      "   ...\n",
      "   [0.00230681 0.00178393 0.00086121]\n",
      "   [0.00230681 0.00175317 0.00086121]\n",
      "   [0.00230681 0.00178393 0.00089196]]\n",
      "\n",
      "  [[0.00359862 0.00358324 0.00330642]\n",
      "   [0.00359862 0.00358324 0.00333718]\n",
      "   [0.00361399 0.00358324 0.00338331]\n",
      "   ...\n",
      "   [0.00230681 0.00178393 0.00084583]\n",
      "   [0.00229143 0.00175317 0.00084583]\n",
      "   [0.00227605 0.00173779 0.00084583]]\n",
      "\n",
      "  [[0.00356786 0.00356786 0.00326028]\n",
      "   [0.00358324 0.00355248 0.00329104]\n",
      "   [0.00359862 0.00356786 0.00335256]\n",
      "   ...\n",
      "   [0.00229143 0.00176855 0.00083045]\n",
      "   [0.00224529 0.00170704 0.00076894]\n",
      "   [0.00224529 0.00167628 0.00078431]]]\n",
      "\n",
      "\n",
      " [[[0.00013841 0.00013841 0.00015379]\n",
      "   [0.00013841 0.00013841 0.00018454]\n",
      "   [0.00016917 0.00015379 0.00016917]\n",
      "   ...\n",
      "   [0.00064591 0.0007228  0.00081507]\n",
      "   [0.00059977 0.0007228  0.00086121]\n",
      "   [0.00055363 0.00075356 0.00095348]]\n",
      "\n",
      "  [[0.00010765 0.00013841 0.00012303]\n",
      "   [0.00012303 0.00016917 0.00015379]\n",
      "   [0.00015379 0.00016917 0.00013841]\n",
      "   ...\n",
      "   [0.00064591 0.00075356 0.00081507]\n",
      "   [0.00061515 0.00081507 0.00096886]\n",
      "   [0.00067666 0.00084583 0.00103037]]\n",
      "\n",
      "  [[0.00010765 0.00013841 0.00015379]\n",
      "   [0.00015379 0.00018454 0.00018454]\n",
      "   [0.00018454 0.00019992 0.00018454]\n",
      "   ...\n",
      "   [0.00059977 0.00076894 0.00086121]\n",
      "   [0.00063053 0.00087659 0.00106113]\n",
      "   [0.00076894 0.0009381  0.0011534 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.0021684  0.00256824 0.00292195]\n",
      "   [0.00219915 0.00258362 0.00293733]\n",
      "   [0.00224529 0.002599   0.00296809]\n",
      "   ...\n",
      "   [0.00092272 0.00095348 0.00106113]\n",
      "   [0.00090734 0.00095348 0.00104575]\n",
      "   [0.00087659 0.00090734 0.00103037]]\n",
      "\n",
      "  [[0.0021684  0.00252211 0.00286044]\n",
      "   [0.00222991 0.00256824 0.00295271]\n",
      "   [0.00227605 0.002599   0.00299885]\n",
      "   ...\n",
      "   [0.0009381  0.00092272 0.00101499]\n",
      "   [0.00092272 0.0009381  0.00101499]\n",
      "   [0.00090734 0.00092272 0.00099962]]\n",
      "\n",
      "  [[0.00215302 0.00242983 0.00275279]\n",
      "   [0.00221453 0.00252211 0.00286044]\n",
      "   [0.00224529 0.002599   0.00295271]\n",
      "   ...\n",
      "   [0.0009381  0.00092272 0.00101499]\n",
      "   [0.00092272 0.0009381  0.00101499]\n",
      "   [0.00090734 0.00092272 0.00099962]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.00344483 0.00063053 0.00118416]\n",
      "   [0.00361399 0.00069204 0.00124567]\n",
      "   [0.00367551 0.00078431 0.00127643]\n",
      "   ...\n",
      "   [0.00172241 0.00196847 0.00190696]\n",
      "   [0.00186082 0.0020915  0.0020915 ]\n",
      "   [0.00189158 0.00213764 0.00213764]]\n",
      "\n",
      "  [[0.00293733 0.00076894 0.00121492]\n",
      "   [0.00327566 0.00070742 0.00112265]\n",
      "   [0.00356786 0.00066128 0.00110727]\n",
      "   ...\n",
      "   [0.00164552 0.0018762  0.00184544]\n",
      "   [0.00167628 0.00190696 0.00189158]\n",
      "   [0.00161476 0.00186082 0.00184544]]\n",
      "\n",
      "  [[0.00235294 0.00107651 0.00132257]\n",
      "   [0.00276817 0.00087659 0.00112265]\n",
      "   [0.00326028 0.00064591 0.00099962]\n",
      "   ...\n",
      "   [0.00153787 0.00170704 0.00164552]\n",
      "   [0.00156863 0.00170704 0.00169166]\n",
      "   [0.00159938 0.00169166 0.00173779]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00109189 0.00076894 0.00056901]\n",
      "   [0.00101499 0.0007228  0.00055363]\n",
      "   [0.00087659 0.00059977 0.00046136]\n",
      "   ...\n",
      "   [0.00235294 0.0020915  0.00232218]\n",
      "   [0.00227605 0.00199923 0.00221453]\n",
      "   [0.00224529 0.00196847 0.00218378]]\n",
      "\n",
      "  [[0.00099962 0.0007228  0.00052288]\n",
      "   [0.00095348 0.00066128 0.00049212]\n",
      "   [0.00090734 0.00059977 0.00047674]\n",
      "   ...\n",
      "   [0.00239908 0.00212226 0.00235294]\n",
      "   [0.00230681 0.00202999 0.00224529]\n",
      "   [0.00215302 0.00190696 0.00207612]]\n",
      "\n",
      "  [[0.00095348 0.00070742 0.00049212]\n",
      "   [0.00092272 0.00066128 0.00047674]\n",
      "   [0.00095348 0.00063053 0.00052288]\n",
      "   ...\n",
      "   [0.0023837  0.00210688 0.00233756]\n",
      "   [0.00246059 0.00219915 0.00239908]\n",
      "   [0.00230681 0.00206075 0.00222991]]]\n",
      "\n",
      "\n",
      " [[[0.00256824 0.00279892 0.00264514]\n",
      "   [0.002599   0.0028143  0.00266052]\n",
      "   [0.00276817 0.00295271 0.00279892]\n",
      "   ...\n",
      "   [0.00290657 0.0031065  0.00284506]\n",
      "   [0.00286044 0.0030296  0.00278354]\n",
      "   [0.00276817 0.00295271 0.00269127]]\n",
      "\n",
      "  [[0.00256824 0.00275279 0.002599  ]\n",
      "   [0.00262976 0.0028143  0.00266052]\n",
      "   [0.00279892 0.00299885 0.00284506]\n",
      "   ...\n",
      "   [0.00292195 0.00313725 0.00287582]\n",
      "   [0.00290657 0.0031065  0.00282968]\n",
      "   [0.00284506 0.00301423 0.00276817]]\n",
      "\n",
      "  [[0.00266052 0.00284506 0.00269127]\n",
      "   [0.00270665 0.00287582 0.00273741]\n",
      "   [0.00278354 0.00299885 0.00282968]\n",
      "   ...\n",
      "   [0.00293733 0.00315263 0.00287582]\n",
      "   [0.00292195 0.00315263 0.00286044]\n",
      "   [0.0028912  0.00307574 0.0028143 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00350634 0.00339869 0.00346021]\n",
      "   [0.00359862 0.00347559 0.00355248]\n",
      "   [0.00362937 0.00349097 0.00356786]\n",
      "   ...\n",
      "   [0.00276817 0.00295271 0.00235294]\n",
      "   [0.00335256 0.00318339 0.00272203]\n",
      "   [0.00319877 0.00312188 0.00290657]]\n",
      "\n",
      "  [[0.00356786 0.00344483 0.00350634]\n",
      "   [0.00361399 0.00346021 0.0035371 ]\n",
      "   [0.00362937 0.00346021 0.00355248]\n",
      "   ...\n",
      "   [0.00292195 0.00306036 0.00269127]\n",
      "   [0.00349097 0.00321415 0.00286044]\n",
      "   [0.00330642 0.00313725 0.00306036]]\n",
      "\n",
      "  [[0.00356786 0.00342945 0.00350634]\n",
      "   [0.00361399 0.00346021 0.00355248]\n",
      "   [0.00362937 0.00346021 0.00355248]\n",
      "   ...\n",
      "   [0.00346021 0.00327566 0.00309112]\n",
      "   [0.00347559 0.00322953 0.00298347]\n",
      "   [0.0033218  0.00318339 0.00326028]]]\n",
      "\n",
      "\n",
      " [[[0.00370627 0.00322953 0.00286044]\n",
      "   [0.00366013 0.00319877 0.00282968]\n",
      "   [0.00362937 0.00316801 0.00279892]\n",
      "   ...\n",
      "   [0.00369089 0.00367551 0.00373702]\n",
      "   [0.00369089 0.00369089 0.00372165]\n",
      "   [0.00370627 0.00367551 0.00372165]]\n",
      "\n",
      "  [[0.00372165 0.00327566 0.00292195]\n",
      "   [0.00364475 0.00319877 0.00282968]\n",
      "   [0.00364475 0.00318339 0.0028143 ]\n",
      "   ...\n",
      "   [0.00369089 0.00370627 0.00376778]\n",
      "   [0.00367551 0.00369089 0.00373702]\n",
      "   [0.00367551 0.00367551 0.00370627]]\n",
      "\n",
      "  [[0.00379854 0.00338331 0.00304498]\n",
      "   [0.00369089 0.00327566 0.00292195]\n",
      "   [0.00362937 0.00321415 0.00286044]\n",
      "   ...\n",
      "   [0.00370627 0.00372165 0.00378316]\n",
      "   [0.00367551 0.00370627 0.0037524 ]\n",
      "   [0.00366013 0.00370627 0.00372165]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00336794 0.00287582 0.00218378]\n",
      "   [0.00279892 0.00233756 0.00170704]\n",
      "   [0.00143022 0.00103037 0.00049212]\n",
      "   ...\n",
      "   [0.00170704 0.00161476 0.00179931]\n",
      "   [0.00184544 0.00173779 0.00193772]\n",
      "   [0.0016609  0.00156863 0.00175317]]\n",
      "\n",
      "  [[0.0035371  0.00304498 0.00247597]\n",
      "   [0.00264514 0.00215302 0.00155325]\n",
      "   [0.00129181 0.00083045 0.00027682]\n",
      "   ...\n",
      "   [0.00143022 0.00135333 0.00146098]\n",
      "   [0.00181469 0.00173779 0.00183007]\n",
      "   [0.00173779 0.0016609  0.00175317]]\n",
      "\n",
      "  [[0.00276817 0.00224529 0.00167628]\n",
      "   [0.00244521 0.00193772 0.00135333]\n",
      "   [0.00147636 0.00096886 0.00041522]\n",
      "   ...\n",
      "   [0.00183007 0.00175317 0.00186082]\n",
      "   [0.00210688 0.00202999 0.00213764]\n",
      "   [0.00196847 0.00189158 0.00196847]]]]\n",
      "--------------------------- [[[[0.08627451 0.08235294 0.06666667]\n",
      "   [0.09411765 0.09019608 0.0745098 ]\n",
      "   [0.10588235 0.10588235 0.09803922]\n",
      "   ...\n",
      "   [0.09019608 0.13333333 0.14901961]\n",
      "   [0.06666667 0.10980392 0.12156863]\n",
      "   [0.05098039 0.09411765 0.09803922]]\n",
      "\n",
      "  [[0.08627451 0.08235294 0.06666667]\n",
      "   [0.09411765 0.09019608 0.0745098 ]\n",
      "   [0.10588235 0.10588235 0.09411765]\n",
      "   ...\n",
      "   [0.09411765 0.1372549  0.14901961]\n",
      "   [0.0745098  0.11764706 0.1254902 ]\n",
      "   [0.05490196 0.09411765 0.10196078]]\n",
      "\n",
      "  [[0.08627451 0.07843137 0.06666667]\n",
      "   [0.09411765 0.09019608 0.07843137]\n",
      "   [0.10196078 0.10588235 0.08627451]\n",
      "   ...\n",
      "   [0.09019608 0.13333333 0.14117647]\n",
      "   [0.07843137 0.11372549 0.1254902 ]\n",
      "   [0.05490196 0.09019608 0.10980392]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.52941176 0.70588235 0.74509804]\n",
      "   [0.49803922 0.67058824 0.70588235]\n",
      "   [0.45490196 0.59607843 0.64705882]\n",
      "   ...\n",
      "   [0.10588235 0.06666667 0.04313725]\n",
      "   [0.09803922 0.0627451  0.03529412]\n",
      "   [0.07843137 0.05490196 0.03137255]]\n",
      "\n",
      "  [[0.55686275 0.7372549  0.77254902]\n",
      "   [0.54509804 0.71764706 0.75686275]\n",
      "   [0.51372549 0.67843137 0.71764706]\n",
      "   ...\n",
      "   [0.09411765 0.05882353 0.03921569]\n",
      "   [0.08235294 0.05098039 0.03137255]\n",
      "   [0.0627451  0.03921569 0.02352941]]\n",
      "\n",
      "  [[0.57254902 0.74509804 0.78039216]\n",
      "   [0.56470588 0.73333333 0.76862745]\n",
      "   [0.5372549  0.70588235 0.74117647]\n",
      "   ...\n",
      "   [0.07843137 0.04705882 0.03529412]\n",
      "   [0.0627451  0.04313725 0.02745098]\n",
      "   [0.05882353 0.03921569 0.02745098]]]\n",
      "\n",
      "\n",
      " [[[0.99215686 0.99215686 0.99607843]\n",
      "   [0.98039216 0.98431373 0.99215686]\n",
      "   [0.98431373 0.98823529 0.99607843]\n",
      "   ...\n",
      "   [0.96470588 0.98039216 0.98823529]\n",
      "   [0.96470588 0.98039216 0.98431373]\n",
      "   [0.99607843 0.99607843 0.99607843]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99607843]\n",
      "   [0.98431373 0.98823529 0.99607843]\n",
      "   [0.98431373 0.98823529 0.99607843]\n",
      "   ...\n",
      "   [0.96470588 0.98039216 0.98823529]\n",
      "   [0.96862745 0.98039216 0.98431373]\n",
      "   [0.99607843 0.99607843 1.        ]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99607843]\n",
      "   [0.98431373 0.98823529 0.99607843]\n",
      "   [0.98431373 0.98823529 0.99607843]\n",
      "   ...\n",
      "   [0.96862745 0.98431373 0.98823529]\n",
      "   [0.97254902 0.98431373 0.98823529]\n",
      "   [0.99607843 0.99607843 1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99215686 0.99607843 0.99607843]\n",
      "   [0.98431373 0.99215686 0.98823529]\n",
      "   [0.98431373 0.99215686 0.98823529]\n",
      "   ...\n",
      "   [0.96470588 0.98039216 0.98431373]\n",
      "   [0.96862745 0.98431373 0.98431373]\n",
      "   [0.99607843 1.         0.99607843]]\n",
      "\n",
      "  [[0.99215686 0.99607843 0.99607843]\n",
      "   [0.98431373 0.99215686 0.98823529]\n",
      "   [0.98431373 0.99215686 0.98823529]\n",
      "   ...\n",
      "   [0.96470588 0.98039216 0.98431373]\n",
      "   [0.96862745 0.98431373 0.98431373]\n",
      "   [0.99607843 1.         0.99607843]]\n",
      "\n",
      "  [[0.99215686 0.99607843 0.99607843]\n",
      "   [0.98431373 0.99215686 0.98823529]\n",
      "   [0.98431373 0.99215686 0.98823529]\n",
      "   ...\n",
      "   [0.96470588 0.98039216 0.98431373]\n",
      "   [0.96862745 0.98431373 0.98431373]\n",
      "   [0.99607843 1.         0.99607843]]]\n",
      "\n",
      "\n",
      " [[[0.99215686 0.99607843 1.        ]\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   ...\n",
      "   [0.90980392 0.90980392 0.90196078]\n",
      "   [0.90196078 0.90980392 0.89803922]\n",
      "   [0.89803922 0.90588235 0.89411765]]\n",
      "\n",
      "  [[0.99215686 0.99607843 1.        ]\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   ...\n",
      "   [0.90980392 0.91372549 0.90588235]\n",
      "   [0.90588235 0.91372549 0.90196078]\n",
      "   [0.90196078 0.90980392 0.89803922]]\n",
      "\n",
      "  [[0.99215686 0.99607843 1.        ]\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   [0.99215686 0.99607843 1.        ]\n",
      "   ...\n",
      "   [0.90980392 0.91764706 0.90588235]\n",
      "   [0.90980392 0.91764706 0.90588235]\n",
      "   [0.90196078 0.90980392 0.89803922]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.62745098 0.40784314 0.2745098 ]\n",
      "   [0.56078431 0.34901961 0.21960784]\n",
      "   [0.50196078 0.29803922 0.18823529]\n",
      "   ...\n",
      "   [0.26666667 0.14117647 0.08627451]\n",
      "   [0.23137255 0.10588235 0.05098039]\n",
      "   [0.23529412 0.10588235 0.05490196]]\n",
      "\n",
      "  [[0.60784314 0.39607843 0.26666667]\n",
      "   [0.52156863 0.30980392 0.18823529]\n",
      "   [0.51372549 0.30980392 0.20392157]\n",
      "   ...\n",
      "   [0.25490196 0.11764706 0.05490196]\n",
      "   [0.26666667 0.1254902  0.0627451 ]\n",
      "   [0.2745098  0.13333333 0.06666667]]\n",
      "\n",
      "  [[0.64313725 0.43529412 0.30588235]\n",
      "   [0.58431373 0.37647059 0.25490196]\n",
      "   [0.54509804 0.34117647 0.23529412]\n",
      "   ...\n",
      "   [0.2627451  0.1254902  0.05882353]\n",
      "   [0.27058824 0.12941176 0.05490196]\n",
      "   [0.28235294 0.1372549  0.0627451 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.96862745 0.97254902 0.96862745]\n",
      "   [0.98431373 0.98431373 0.98431373]\n",
      "   [0.99215686 0.99215686 0.99215686]\n",
      "   ...\n",
      "   [0.96470588 0.96470588 0.96470588]\n",
      "   [0.95686275 0.95686275 0.96078431]\n",
      "   [0.95294118 0.95294118 0.94509804]]\n",
      "\n",
      "  [[0.97647059 0.98039216 0.97647059]\n",
      "   [0.98431373 0.98823529 0.98823529]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.96862745 0.96862745 0.96862745]\n",
      "   [0.96470588 0.96470588 0.96078431]\n",
      "   [0.95294118 0.95294118 0.95294118]]\n",
      "\n",
      "  [[0.98039216 0.98039216 0.98039216]\n",
      "   [0.99215686 0.99607843 0.99215686]\n",
      "   [1.         0.99607843 0.99607843]\n",
      "   ...\n",
      "   [0.97254902 0.97254902 0.97254902]\n",
      "   [0.96470588 0.96470588 0.96470588]\n",
      "   [0.96078431 0.96078431 0.96470588]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.88235294 0.87843137 0.87843137]\n",
      "   [0.90588235 0.90588235 0.90588235]\n",
      "   [0.93333333 0.93333333 0.92941176]\n",
      "   ...\n",
      "   [0.28235294 0.21960784 0.16470588]\n",
      "   [0.38823529 0.36470588 0.31372549]\n",
      "   [0.45882353 0.45490196 0.42352941]]\n",
      "\n",
      "  [[0.86666667 0.87058824 0.86666667]\n",
      "   [0.88627451 0.89019608 0.89019608]\n",
      "   [0.91372549 0.91372549 0.91372549]\n",
      "   ...\n",
      "   [0.24705882 0.18431373 0.1254902 ]\n",
      "   [0.36078431 0.32941176 0.26666667]\n",
      "   [0.43137255 0.43137255 0.39215686]]\n",
      "\n",
      "  [[0.83921569 0.85882353 0.84313725]\n",
      "   [0.8627451  0.87843137 0.86666667]\n",
      "   [0.89803922 0.89803922 0.90196078]\n",
      "   ...\n",
      "   [0.21568627 0.16862745 0.10980392]\n",
      "   [0.32156863 0.29411765 0.23137255]\n",
      "   [0.40392157 0.40392157 0.36470588]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [0.95294118 0.96078431 0.95294118]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [0.95294118 0.96470588 0.96078431]\n",
      "   [0.99215686 0.99607843 0.99607843]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [0.94509804 0.96078431 0.95686275]\n",
      "   [0.98039216 0.98823529 0.98431373]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8745098  0.83137255 0.76078431]\n",
      "   [0.88235294 0.83921569 0.76862745]\n",
      "   [0.88235294 0.83921569 0.76862745]\n",
      "   ...\n",
      "   [0.72941176 0.69803922 0.41568627]\n",
      "   [0.7254902  0.69411765 0.40392157]\n",
      "   [0.72941176 0.69019608 0.40392157]]\n",
      "\n",
      "  [[0.85882353 0.81568627 0.74117647]\n",
      "   [0.87058824 0.83137255 0.75686275]\n",
      "   [0.85882353 0.81568627 0.74509804]\n",
      "   ...\n",
      "   [0.73333333 0.69803922 0.41960784]\n",
      "   [0.72941176 0.69411765 0.40784314]\n",
      "   [0.72156863 0.68235294 0.40392157]]\n",
      "\n",
      "  [[0.84705882 0.8        0.7254902 ]\n",
      "   [0.84705882 0.8        0.72156863]\n",
      "   [0.84313725 0.79215686 0.71764706]\n",
      "   ...\n",
      "   [0.7372549  0.69803922 0.41960784]\n",
      "   [0.72156863 0.68235294 0.40392157]\n",
      "   [0.71372549 0.6745098  0.39215686]]]\n",
      "\n",
      "\n",
      " [[[0.00392157 0.01176471 0.01176471]\n",
      "   [0.00392157 0.01176471 0.01568627]\n",
      "   [0.00784314 0.00784314 0.01568627]\n",
      "   ...\n",
      "   [0.10588235 0.07058824 0.03137255]\n",
      "   [0.10588235 0.07058824 0.03137255]\n",
      "   [0.0745098  0.05882353 0.03529412]]\n",
      "\n",
      "  [[0.00784314 0.01960784 0.01960784]\n",
      "   [0.00784314 0.01960784 0.02745098]\n",
      "   [0.01568627 0.01568627 0.02352941]\n",
      "   ...\n",
      "   [0.24313725 0.17254902 0.05098039]\n",
      "   [0.24313725 0.15686275 0.03529412]\n",
      "   [0.14901961 0.09803922 0.03137255]]\n",
      "\n",
      "  [[0.00784314 0.01176471 0.02352941]\n",
      "   [0.00784314 0.01176471 0.03137255]\n",
      "   [0.00784314 0.01176471 0.02745098]\n",
      "   ...\n",
      "   [0.24705882 0.17647059 0.0627451 ]\n",
      "   [0.24705882 0.17254902 0.04313725]\n",
      "   [0.14901961 0.09803922 0.03529412]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.36862745 0.28235294 0.19215686]\n",
      "   [0.57647059 0.43529412 0.28235294]\n",
      "   [0.58431373 0.45882353 0.29803922]\n",
      "   ...\n",
      "   [0.61176471 0.50980392 0.36470588]\n",
      "   [0.61176471 0.50588235 0.36862745]\n",
      "   [0.38823529 0.3254902  0.25098039]]\n",
      "\n",
      "  [[0.38431373 0.29803922 0.20392157]\n",
      "   [0.59607843 0.45098039 0.29019608]\n",
      "   [0.58823529 0.43921569 0.28235294]\n",
      "   ...\n",
      "   [0.60784314 0.49411765 0.34901961]\n",
      "   [0.62352941 0.52156863 0.38039216]\n",
      "   [0.41176471 0.36078431 0.28235294]]\n",
      "\n",
      "  [[0.16470588 0.1372549  0.10980392]\n",
      "   [0.25098039 0.19607843 0.14509804]\n",
      "   [0.23921569 0.18823529 0.12941176]\n",
      "   ...\n",
      "   [0.25882353 0.21568627 0.16470588]\n",
      "   [0.26666667 0.22745098 0.17647059]\n",
      "   [0.18039216 0.15686275 0.1254902 ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_val)\n",
    "print('---------------------------',X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 5)\n",
      "(2353, 5)\n",
      "(785, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2353, 128, 128, 3)\n",
      "2353\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "#np_load_old = np.load\n",
    "#np.load = lambda *a, **k : np_load_old(*a, allow_pickle=True, **k)\n",
    "#np.load = np_load_old\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "X_train, X_test, X_val , y_train, y_test, y_val= np.load('C:/Users/ICT01_20/Desktop/food-5-2.npy',allow_pickle=True\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n",
    "X_val = X_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_20\\Anaconda3\\envs\\a\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "with K.tf_ops.device('/device:GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './test'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/food5.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               16777472  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 16,798,149\n",
      "Trainable params: 16,798,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2353 samples, validate on 785 samples\n",
      "Epoch 1/20\n",
      "2353/2353 [==============================] - 13s 5ms/step - loss: 0.0447 - acc: 0.9860 - val_loss: 0.3291 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00001: val_loss improved from 3.01411 to 0.32909, saving model to ./test/food5.model\n",
      "Epoch 2/20\n",
      "2353/2353 [==============================] - 13s 5ms/step - loss: 0.0827 - acc: 0.9703 - val_loss: 0.3479 - val_acc: 0.8917\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.32909\n",
      "Epoch 3/20\n",
      "2353/2353 [==============================] - 12s 5ms/step - loss: 0.0555 - acc: 0.9839 - val_loss: 0.3302 - val_acc: 0.9045\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32909\n",
      "Epoch 4/20\n",
      "2353/2353 [==============================] - 12s 5ms/step - loss: 0.0621 - acc: 0.9754 - val_loss: 0.3982 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.32909\n",
      "Epoch 5/20\n",
      "2353/2353 [==============================] - 13s 5ms/step - loss: 0.0453 - acc: 0.9839 - val_loss: 0.4645 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.32909\n",
      "Epoch 6/20\n",
      "2353/2353 [==============================] - 13s 5ms/step - loss: 0.0899 - acc: 0.9732 - val_loss: 0.4042 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32909\n",
      "Epoch 7/20\n",
      "2353/2353 [==============================] - 13s 5ms/step - loss: 0.0499 - acc: 0.9822 - val_loss: 0.3747 - val_acc: 0.8943\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.32909\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=10, epochs=20, validation_data=(X_test, y_test), \n",
    "                    callbacks=[checkpoint, early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
